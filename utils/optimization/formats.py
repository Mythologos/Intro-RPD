from utils.models.constants import EmbeddingType, EncoderType


def get_numeric_string(number: int, places: int):
    numeric_string: str = str(number)
    while len(numeric_string) < places:
        numeric_string = "0" + numeric_string
    return numeric_string


TRAINING_OUTPUT_FORMATS: dict[tuple[str, str], str] = {
    (EmbeddingType.LEARNED, EncoderType.LSTM):
        "python3 neural_tagger.py train learned lstm "
        "--model-location {} "
        "--model-name {} "
        "--results-directory {} "
        "--training-filename {} "
        "--validation-filename {} "
        "--collection-format {} "
        "--dataset {} "
        "--embedding-size {} "
        "--epochs {} "
        "--hidden-size {} "
        "--layers {} "
        "--learning-rate {} "
        "{} "   # for --lemmatization or --no-lemmatization
        "--link {} "
        "--optimizer {} "
        "--patience {} "
        "--random-seed {} "
        "--replacement-probability {} "
        "--replacement-strategy {} "
        "--scoring-mode {} "
        "--stratum-count {} "
        "--tagset {} "
        "--weight-decay {}",
    (EmbeddingType.WORD, EncoderType.LSTM):
        "python3 neural_tagger.py train word lstm "
        "--model-location {} "
        "--model-name {} "
        "--results-directory {} "
        "--training-filename {} "
        "--validation-filename {} "
        "--collection-format {} "
        "--dataset {} "
        "--embedding-filepath resources/word2vec/{} "
        "--embedding-size {} "
        "--epochs {} "
        "{} "   # for --frozen-embeddings or --no-frozen-embeddings
        "--hidden-size {} "
        "--layers {} "
        "--learning-rate {} "
        "{} "   # for --lemmatization or --no-lemmatization
        "--link {} "
        "--optimizer {} "
        "--patience {} "
        "--random-seed {} "
        "--scoring-mode {} "
        "--stratum-count {} "
        "--tagset {} "
        "--weight-decay {}",
    (EmbeddingType.LATIN_LEARNED_SUBWORD, EncoderType.TRANSFORMER):
        "python3 neural_tagger.py train latin-learned-subword transformer "
        "--model-location {} "
        "--model-name {} "
        "--results-directory {} "
        "--training-filename {} "
        "--validation-filename {} "
        "--activation {} "
        "--blender {} "
        "--collection-format {} "
        "--dataset {} "
        "--dropout {} "
        "--embedding-size {} "
        "--epochs {} "
        "--heads {} "
        "--hidden-size {} "
        "--layers {} "
        "--learning-rate {} "
        "{} "   # --lemmatization or --no-lemmatization
        "--link {} "
        "--optimizer {} "
        "--patience {} "
        "--random-seed {} "
        "--scoring-mode {} "
        "--stratum-count {} "
        "--tagset {} "
        "--tokenizer-filepath resources/latin-bert/subword_tokenizer_latin/latin.subword.encoder "
        "--weight-decay {}",
    (EmbeddingType.LATIN_BERT, EncoderType.IDENTITY):
        "python3 neural_tagger.py train latin-bert identity "
        "--model-location {} "
        "--model-name {} "
        "--results-directory {} "
        "--training-filename {} "
        "--validation-filename {} "
        "--blender {} "
        "--collection-format {} "
        "--dataset {} "
        "--epochs {} "
        "{} "   # for --frozen-embeddings or --no-frozen-embeddings
        "--learning-rate {} "
        "{} "   # for --lemmatization or --no-lemmatization
        "--link {} "
        "--optimizer {} "
        "--patience {} "
        "--pretrained-filepath resources/latin-bert/latin_bert "
        "--random-seed {} "
        "--scoring-mode {} "
        "--stratum-count {} "
        "--tagset {} "
        "--tokenizer-filepath resources/latin-bert/subword_tokenizer_latin/latin.subword.encoder "
        "--weight-decay {}",
    (EmbeddingType.LATIN_BERT, EncoderType.LSTM):
        "python3 neural_tagger.py train latin-bert lstm "
        "--model-location {} "
        "--model-name {} "
        "--results-directory {} "
        "--training-filename {} "
        "--validation-filename {} "
        "--blender {} "
        "--collection-format {} "
        "--dataset {} "
        "--epochs {} "
        "{} "  # for --frozen-embeddings or --no-frozen-embeddings
        "--hidden-size {} "
        "--layers {} "
        "--learning-rate {} "
        "{} "  # for --lemmatization or --no-lemmatization
        "--link {} "
        "--optimizer {} "
        "--patience {} "
        "--pretrained-filepath resources/latin-bert/latin_bert "
        "--random-seed {} "
        "--scoring-mode {} "
        "--stratum-count {} "
        "--tagset {} "
        "--tokenizer-filepath resources/latin-bert/subword_tokenizer_latin/latin.subword.encoder "
        "--weight-decay {}",
    (EmbeddingType.LATIN_BERT, EncoderType.TRANSFORMER):
        "python3 neural_tagger.py train latin-bert transformer "
        "--model-location {} "
        "--model-name {} "
        "--results-directory {} "
        "--training-filename {} "
        "--validation-filename {} "
        "--activation {} "
        "--blender {} "
        "--collection-format {} "
        "--dataset {} "
        "--dropout {} "
        "--epochs {} "
        "{} "   # for --frozen-embeddings or --no-frozen-embeddings
        "--heads {} "
        "--hidden-size {} "
        "--layers {} "
        "--learning-rate {} "
        "{} "  # for --lemmatization or --no-lemmatization
        "--link {} "
        "--optimizer {} "
        "--patience {} "
        "--pretrained-filepath resources/latin-bert/latin_bert "
        "--random-seed {} "
        "--scoring-mode {} "
        "--stratum-count {} "
        "--tagset {} "
        "--tokenizer-filepath "
        "resources/latin-bert/subword_tokenizer_latin/latin.subword.encoder "
        "--weight-decay {}",
    (EmbeddingType.CHINESE_BERT, EncoderType.IDENTITY):
        "python3 neural_tagger.py train chinese-bert identity "
        "--model-location {} "
        "--model-name {} "
        "--results-directory {} "
        "--training-filename {} "
        "--validation-filename {} "
        "--blender {} "
        "--collection-format {} "
        "--dataset {} "
        "--epochs {} "
        "{} "  # for --frozen-embeddings or --no-frozen-embeddings
        "--learning-rate {} "
        "{} "  # for --lemmatization or --no-lemmatization
        "--link {} "
        "--optimizer {} "
        "--patience {} "
        "--pretrained-filepath resources/chinese-bert-wwm "
        "--random-seed {} "
        "--scoring-mode {} "
        "--stratum-count {} "
        "--tagset {} "
        "--tokenizer-filepath resources/chinese-bert-wwm "
        "--weight-decay {}",
    (EmbeddingType.CHINESE_BERT, EncoderType.LSTM):
        "python3 neural_tagger.py train chinese-bert lstm "
        "--model-location {} "
        "--model-name {} "
        "--results-directory {} "
        "--training-filename {} "
        "--validation-filename {} "
        "--blender {} "
        "--collection-format {} "
        "--dataset {} "
        "--epochs {} "
        "{} "  # for --frozen-embeddings or --no-frozen-embeddings
        "--hidden-size {} "
        "--layers {} "
        "--learning-rate {} "
        "{} "  # for --lemmatization or --no-lemmatization
        "--link {} "
        "--optimizer {} "
        "--patience {} "
        "--pretrained-filepath resources/chinese-bert-wwm "
        "--random-seed {} "
        "--scoring-mode {} "
        "--stratum-count {} "
        "--tagset {} "
        "--tokenizer-filepath resources/chinese-bert-wwm "
        "--weight-decay {}",
    (EmbeddingType.CHINESE_BERT, EncoderType.TRANSFORMER):
        "python3 neural_tagger.py train chinese-bert transformer "
        "--model-location {} "
        "--model-name {} "
        "--results-directory {} "
        "--training-filename {} "
        "--validation-filename {} "
        "--activation {} "
        "--blender {} "
        "--collection-format {} "
        "--dataset {} "
        "--dropout {} "
        "--epochs {} "
        "{} "  # for --frozen-embeddings or --no-frozen-embeddings
        "--heads {} "
        "--hidden-size {} "
        "--layers {} "
        "--learning-rate {} "
        "{} "  # for --lemmatization or --no-lemmatization
        "--link {} "
        "--optimizer {} "
        "--patience {} "
        "--pretrained-filepath resources/chinese-bert-wwm "
        "--random-seed {} "
        "--scoring-mode {} "
        "--stratum-count {} "
        "--tagset {} "
        "--tokenizer-filepath resources/chinese-bert-wwm "
        "--weight-decay {}"
}

EVALUATION_OUTPUT_FORMATS: dict[tuple[str, str], str] = {
    (EmbeddingType.LEARNED, EncoderType.LSTM):
        "python3 neural_tagger.py evaluate learned lstm "
        "--model-location {0} "
        "--model-name {1} "
        "--results-directory {2} "
        "--test-filename {3} "
        "--evaluation-partition {4} "
        "--collection-format {5} "
        "--dataset {6} "
        "{12} "
        "--link {13} "
        "--random-seed {16} "
        "--scoring-mode {19} "
        "--stratum-count {20} "
        "--tagset {21}",
    (EmbeddingType.WORD, EncoderType.LSTM):
        "python3 neural_tagger.py evaluate word lstm "
        "--model-location {0} "
        "--model-name {1} "
        "--results-directory {2} "
        "--test-filename {3} "
        "--evaluation-partition {4} "
        "--collection-format {5} "
        "--dataset {6} "
        "{14} "
        "--link {15} "
        "--random-seed {18} "
        "--scoring-mode {19} "
        "--stratum-count {20} "
        "--tagset {21}",
    (EmbeddingType.LATIN_LEARNED_SUBWORD, EncoderType.TRANSFORMER):
        "python3 neural_tagger.py evaluate latin-learned-subword transformer "
        "--model-location {0} "
        "--model-name {1} "
        "--results-directory {2} "
        "--test-filename {3} "
        "--evaluation-partition {4} "
        "--collection-format {7} "
        "--dataset {8} "
        "{16} "
        "--link {17} "
        "--random-seed {20} "
        "--scoring-mode {21} "
        "--stratum-count {22} "
        "--tagset {23}",
    (EmbeddingType.LATIN_BERT, EncoderType.IDENTITY):
        "python3 neural_tagger.py evaluate latin-bert identity "
        "--model-location {0} "
        "--model-name {1} "
        "--results-directory {2} "
        "--test-filename {3} "
        "--evaluation-partition {4} "
        "--collection-format {6} "
        "--dataset {7} "
        "{11} "
        "--link {12} "
        "--random-seed {15} "
        "--scoring-mode {16} "
        "--stratum-count {17} "
        "--tagset {18}",
    (EmbeddingType.LATIN_BERT, EncoderType.LSTM):
        "python3 neural_tagger.py evaluate latin-bert lstm "
        "--model-location {0} "
        "--model-name {1} "
        "--results-directory {2} "
        "--test-filename {3} "
        "--evaluation-partition {4} "
        "--collection-format {6} "
        "--dataset {7} "
        "{13} "
        "--link {14} "
        "--random-seed {17} "
        "--scoring-mode {18} "
        "--stratum-count {19} "
        "--tagset {20}",
    (EmbeddingType.LATIN_BERT, EncoderType.TRANSFORMER):
        "python3 neural_tagger.py evaluate latin-bert transformer "
        "--model-location {0} "
        "--model-name {1} "
        "--results-directory {2} "
        "--test-filename {3} "
        "--evaluation-partition {4} "
        "--collection-format {7} "
        "--dataset {8} "
        "{16} "
        "--link {17} "
        "--random-seed {20} "
        "--scoring-mode {21} "
        "--stratum-count {22} "
        "--tagset {23}",
    (EmbeddingType.CHINESE_BERT, EncoderType.IDENTITY):
        "python3 neural_tagger.py evaluate chinese-bert identity "
        "--model-location {0} "
        "--model-name {1} "
        "--results-directory {2} "
        "--test-filename {3} "
        "--evaluation-partition {4} "
        "--collection-format {6} "
        "--dataset {7} "
        "{11} "
        "--link {12} "
        "--random-seed {15} "
        "--scoring-mode {16} "
        "--stratum-count {17} "
        "--tagset {18}",
    (EmbeddingType.CHINESE_BERT, EncoderType.LSTM):
        "python3 neural_tagger.py evaluate chinese-bert lstm "
        "--model-location {0} "
        "--model-name {1} "
        "--results-directory {2} "
        "--test-filename {3} "
        "--evaluation-partition {4} "
        "--collection-format {6} "
        "--dataset {7} "
        "{13} "
        "--link {14} "
        "--random-seed {17} "
        "--scoring-mode {18} "
        "--stratum-count {19} "
        "--tagset {20}",
    (EmbeddingType.CHINESE_BERT, EncoderType.TRANSFORMER):
        "python3 neural_tagger.py evaluate chinese-bert transformer "
        "--model-location {0} "
        "--model-name {1} "
        "--results-directory {2} "
        "--test-filename {3} "
        "--evaluation-partition {4} "
        "--collection-format {7} "
        "--dataset {8} "
        "{16} "
        "--link {17} "
        "--random-seed {20} "
        "--scoring-mode {21} "
        "--stratum-count {22} "
        "--tagset {23}"
}

BASH_FORMAT: str = "#!/bin/bash\n" \
                   "# Produced with random seed {0}...\n\n" \
                   "# Main Commands:\n" \
                   "conda activate ParaLatin\n" \
                   "{1}"
